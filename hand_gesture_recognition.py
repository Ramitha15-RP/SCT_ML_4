# -*- coding: utf-8 -*-
"""hand_gesture_recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o4QZvRG8vMNjI9jlYh20T6Qn_PlHIK8q
"""

!pip install tensorflow opencv-python matplotlib seaborn scikit-learn mediapipe --quiet

import tensorflow as tf
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import mediapipe as mp
print("All packages installed successfully!")

!pip install tensorflow opencv-python matplotlib seaborn scikit-learn mediapipe --quiet

import tensorflow as tf
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import mediapipe as mp

print("All packages installed and imported successfully!")

def load_hand_gesture_data():
    """
    Create sample hand gesture data for demonstration
    In real scenario, replace this with actual dataset loading
    """
    print("Creating sample hand gesture data...")

    class_names = ['fist', 'open_palm', 'thumbs_up', 'peace', 'ok_sign', 'pointing']

    images = []
    labels = []

    for class_idx, gesture_name in enumerate(class_names):
        for i in range(150):

            img = np.random.rand(128, 128, 3) * 0.2

            if gesture_name == 'fist':

                cv2.circle(img, (64, 64), 35, (1.0, 1.0, 1.0), -1)

            elif gesture_name == 'open_palm':

                points = np.array([
                    [64, 25], [50, 50], [35, 50], [45, 70],
                    [40, 95], [64, 80], [88, 95], [83, 70],
                    [93, 50], [78, 50]
                ], np.int32)
                cv2.fillPoly(img, [points], (1.0, 1.0, 1.0))

            elif gesture_name == 'thumbs_up':

                cv2.rectangle(img, (45, 45), (83, 83), (1.0, 1.0, 1.0), -1)
                cv2.rectangle(img, (55, 25), (73, 45), (1.0, 1.0, 1.0), -1)

            elif gesture_name == 'peace':

                cv2.rectangle(img, (40, 40), (88, 88), (1.0, 1.0, 1.0), -1)
                cv2.rectangle(img, (45, 20), (55, 40), (1.0, 1.0, 1.0), -1)
                cv2.rectangle(img, (73, 20), (83, 40), (1.0, 1.0, 1.0), -1)

            elif gesture_name == 'ok_sign':

                cv2.circle(img, (64, 64), 30, (1.0, 1.0, 1.0), -1)
                cv2.circle(img, (64, 64), 12, (0.0, 0.0, 0.0), -1)

            elif gesture_name == 'pointing':

                cv2.rectangle(img, (45, 45), (83, 83), (1.0, 1.0, 1.0), -1)
                cv2.rectangle(img, (60, 20), (68, 45), (1.0, 1.0, 1.0), -1)


            noise = np.random.normal(0, 0.05, img.shape)
            img = np.clip(img + noise, 0, 1)

            images.append(img)
            labels.append(class_idx)

    return np.array(images), np.array(labels), class_names


X, y, class_names = load_hand_gesture_data()

print(f"Dataset created: {X.shape[0]} images, {len(class_names)} classes")
print(f"Class names: {class_names}")
print(f"Image shape: {X[0].shape}")

plt.figure(figsize=(15, 10))

for class_idx in range(len(class_names)):
    class_indices = np.where(y == class_idx)[0]
    sample_idx = class_indices[0]

    plt.subplot(2, 3, class_idx + 1)
    plt.imshow(X[sample_idx])
    plt.title(f'{class_names[class_idx]} (Class {class_idx})')
    plt.axis('off')

plt.suptitle('Sample Hand Gestures from Each Class', fontsize=16, y=0.95)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
unique, counts = np.unique(y, return_counts=True)
plt.bar([class_names[i] for i in unique], counts, color='skyblue')
plt.title('Class Distribution in Hand Gesture Dataset')
plt.xlabel('Gesture Classes')
plt.ylabel('Number of Images')
plt.xticks(rotation=45)
plt.grid(axis='y', alpha=0.3)
plt.show()

def preprocess_data(images, labels, test_size=0.2, val_size=0.2):
    """Split data into train, validation, and test sets"""

    y_categorical = to_categorical(labels, num_classes=len(class_names))

    X_train_val, X_test, y_train_val, y_test = train_test_split(
        images, y_categorical, test_size=test_size, random_state=42, stratify=labels
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_val, y_train_val,
        test_size=val_size,
        random_state=42,
        stratify=np.argmax(y_train_val, axis=1)
    )

    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Validation set: {X_val.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")

    return X_train, X_val, X_test, y_train, y_val, y_test

X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(X, y)

datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2],
    fill_mode='nearest'
)

print("Data preprocessing completed!")

def create_gesture_model(input_shape, num_classes):
    """Create CNN model for hand gesture recognition"""

    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.25),

        Conv2D(64, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.25),

        Conv2D(128, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.25),

        Conv2D(256, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.25),

        Flatten(),
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])

    return model

input_shape = X_train[0].shape
num_classes = len(class_names)

model = create_gesture_model(input_shape, num_classes)

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("Model created successfully!")
model.summary()

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

callbacks = [
    EarlyStopping(patience=10, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(patience=5, factor=0.5, verbose=1)
]

print("Starting model training...")

history = model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    epochs=30,
    validation_data=(X_val, y_val),
    callbacks=callbacks,
    verbose=1
)

print("Model training completed!")

def evaluate_model(model, X_test, y_test, class_names):
    """Comprehensive model evaluation"""

    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_test, axis=1)

    test_accuracy = np.mean(y_pred_classes == y_true_classes)

    print(f"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    print("\n Classification Report:")
    print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))

    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(y_true_classes, y_pred_classes)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names,
                yticklabels=class_names)
    plt.title('Confusion Matrix - Hand Gesture Recognition')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    return test_accuracy

test_accuracy = evaluate_model(model, X_test, y_test, class_names)

plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

model.save('hand_gesture_model.keras')
print("Model saved as 'hand_gesture_model.keras'")

import json
with open('gesture_classes.json', 'w') as f:
    json.dump(class_names, f)
print("Class names saved as 'gesture_classes.json'")

print("Hand Gesture Recognition Model Completed Successfully!")